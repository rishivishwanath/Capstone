{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Gesture Recognition using OpenCV and MediaPipe\n",
    "\n",
    "This notebook demonstrates how to create a static gesture recognition system using:\n",
    "- OpenCV for webcam capture and image processing\n",
    "- MediaPipe for detecting body pose and hand landmarks\n",
    "- Scikit-learn for training a classifier\n",
    "\n",
    "You'll be able to collect your own training data from webcam, extract features, and train a model to recognize your gestures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\aiml\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: mediapipe in c:\\aiml\\lib\\site-packages (0.10.14)\n",
      "Requirement already satisfied: numpy in c:\\aiml\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\aiml\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pandas in c:\\aiml\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\aiml\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: absl-py in c:\\aiml\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\aiml\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\aiml\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\aiml\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in c:\\aiml\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\aiml\\lib\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\aiml\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\aiml\\lib\\site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\aiml\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\aiml\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\aiml\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\aiml\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\aiml\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\aiml\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\aiml\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\aiml\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\aiml\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\aiml\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\aiml\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\aiml\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\aiml\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\aiml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\aiml\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\aiml\\lib\\site-packages (from jax->mediapipe) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum in c:\\aiml\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\aiml\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python mediapipe numpy scikit-learn pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose and Hands solutions\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Define drawing specs\n",
    "pose_drawing_spec = mp_drawing.DrawingSpec(thickness=2, circle_radius=2, color=(0, 255, 0))\n",
    "hand_drawing_spec = mp_drawing.DrawingSpec(thickness=2, circle_radius=2, color=(0, 0, 255))\n",
    "connection_drawing_spec = mp_drawing.DrawingSpec(thickness=1, color=(255, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(results):\n",
    "    \"\"\"Extract pose and hand landmarks from MediaPipe results\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Extract pose landmarks (33 landmarks × 3 coordinates = 99 features)\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            features.extend([landmark.x, landmark.y, landmark.z])\n",
    "    else:\n",
    "        # If no pose detected, fill with zeros\n",
    "        features.extend([0.0] * (33 * 3))\n",
    "    \n",
    "    # Extract left hand landmarks (21 landmarks × 3 coordinates = 63 features)\n",
    "    left_hand_found = False\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            handedness = results.multi_handedness[hand_idx].classification[0].label\n",
    "            if handedness == \"Left\":\n",
    "                left_hand_found = True\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    features.extend([landmark.x, landmark.y, landmark.z])\n",
    "                break\n",
    "    \n",
    "    if not left_hand_found:\n",
    "        # If no left hand detected, fill with zeros\n",
    "        features.extend([0.0] * (21 * 3))\n",
    "    \n",
    "    # Extract right hand landmarks (21 landmarks × 3 coordinates = 63 features)\n",
    "    right_hand_found = False\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            handedness = results.multi_handedness[hand_idx].classification[0].label\n",
    "            if handedness == \"Right\":\n",
    "                right_hand_found = True\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    features.extend([landmark.x, landmark.y, landmark.z])\n",
    "                break\n",
    "    \n",
    "    if not right_hand_found:\n",
    "        # If no right hand detected, fill with zeros\n",
    "        features.extend([0.0] * (21 * 3))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def normalize_landmarks(features):\n",
    "    \"\"\"Normalize landmarks relative to the body to make them scale and position invariant\"\"\"\n",
    "    normalized_features = []\n",
    "    \n",
    "    # If we have valid pose data (check if not all zeros)\n",
    "    if not all(f == 0 for f in features[:99]):\n",
    "        # Use shoulders as reference points for normalization\n",
    "        left_shoulder_x = features[33]  # left_shoulder x (11th landmark)\n",
    "        left_shoulder_y = features[34]  # left_shoulder y\n",
    "        right_shoulder_x = features[36]  # right_shoulder x (12th landmark)\n",
    "        right_shoulder_y = features[37]  # right_shoulder y\n",
    "        \n",
    "        # Calculate shoulder width for scale normalization\n",
    "        shoulder_width = np.sqrt((right_shoulder_x - left_shoulder_x)**2 + \n",
    "                                (right_shoulder_y - left_shoulder_y)**2)\n",
    "        \n",
    "        # Calculate midpoint between shoulders as reference point\n",
    "        ref_x = (left_shoulder_x + right_shoulder_x) / 2\n",
    "        ref_y = (left_shoulder_y + right_shoulder_y) / 2\n",
    "        \n",
    "        # Normalize all landmarks\n",
    "        for i in range(0, len(features), 3):\n",
    "            if shoulder_width > 0:\n",
    "                # Normalize x and y coordinates relative to shoulder midpoint and width\n",
    "                normalized_features.append((features[i] - ref_x) / shoulder_width)\n",
    "                normalized_features.append((features[i+1] - ref_y) / shoulder_width)\n",
    "                # Keep z values but normalize by shoulder width for scale invariance\n",
    "                normalized_features.append(features[i+2] / shoulder_width)\n",
    "            else:\n",
    "                # If reference points are invalid, keep original values\n",
    "                normalized_features.extend(features[i:i+3])\n",
    "    else:\n",
    "        # If no valid pose data, return original features\n",
    "        normalized_features = features\n",
    "        \n",
    "    return normalized_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Collection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_training_data(gesture_name, num_samples=20, countdown=10):\n",
    "    \"\"\"Collect training data for a specific gesture\"\"\"\n",
    "    # Create directory for saving data if it doesn't exist\n",
    "    os.makedirs('gesture_data', exist_ok=True)\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    sample_count = 0\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Countdown before starting data collection\n",
    "    start_time = time.time()\n",
    "    countdown_complete = False\n",
    "    \n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, \\\n",
    "         mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Failed to capture image from webcam.\")\n",
    "                break\n",
    "                \n",
    "            # Flip the image horizontally for a selfie-view display\n",
    "            image = cv2.flip(image, 1)\n",
    "            \n",
    "            # Convert the BGR image to RGB\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process the image for pose and hand detection\n",
    "            pose_results = pose.process(image_rgb)\n",
    "            hands_results = hands.process(image_rgb)\n",
    "            \n",
    "            # Copy original image for display\n",
    "            display_image = image.copy()\n",
    "            \n",
    "            # Check countdown status\n",
    "            current_time = time.time()\n",
    "            elapsed_time = current_time - start_time\n",
    "            \n",
    "            if not countdown_complete:\n",
    "                remaining = countdown - int(elapsed_time)\n",
    "                if remaining > 0:\n",
    "                    # Display countdown\n",
    "                    cv2.putText(display_image, f\"Get ready! Starting in {remaining}...\", (50, 50),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                else:\n",
    "                    countdown_complete = True\n",
    "            \n",
    "            # If countdown is complete, start collecting data\n",
    "            if countdown_complete:\n",
    "                # Create a combined result object to pass to the feature extraction function\n",
    "                combined_results = type('CombinedResults', (), {\n",
    "                    'pose_landmarks': pose_results.pose_landmarks if pose_results.pose_landmarks else None,\n",
    "                    'multi_hand_landmarks': hands_results.multi_hand_landmarks if hands_results.multi_hand_landmarks else None,\n",
    "                    'multi_handedness': hands_results.multi_handedness if hands_results.multi_handedness else None\n",
    "                })\n",
    "                \n",
    "                # Extract landmarks\n",
    "                features = extract_landmarks(combined_results)\n",
    "                \n",
    "                # Normalize landmarks\n",
    "                normalized_features = normalize_landmarks(features)\n",
    "                \n",
    "                # Append to our dataset\n",
    "                data.append(normalized_features)\n",
    "                labels.append(gesture_name)\n",
    "                sample_count += 1\n",
    "                \n",
    "                # Display progress\n",
    "                cv2.putText(display_image, f\"Collecting: {gesture_name} - {sample_count}/{num_samples}\", (50, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Draw landmarks on the image\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    display_image,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=pose_drawing_spec,\n",
    "                    connection_drawing_spec=connection_drawing_spec)\n",
    "            \n",
    "            if hands_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        display_image,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        landmark_drawing_spec=hand_drawing_spec,\n",
    "                        connection_drawing_spec=connection_drawing_spec)\n",
    "            \n",
    "            # Show the image\n",
    "            cv2.imshow('MediaPipe Pose and Hands', display_image)\n",
    "            \n",
    "            # Check if we've collected enough samples\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            # Exit if ESC key is pressed\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Save collected data\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"gesture_data/{gesture_name}_{timestamp}.pkl\"\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump({'data': data, 'labels': labels}, f)\n",
    "    \n",
    "    print(f\"Collected {sample_count} samples for gesture '{gesture_name}'\")\n",
    "    print(f\"Data saved to {filename}\")\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Collect Training Data\n",
    "\n",
    "Run the cell below for each gesture you want to train. Change the `gesture_name` and `num_samples` parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 100 samples for gesture 'you'\n",
      "Data saved to gesture_data/you_20250318_175340.pkl\n"
     ]
    }
   ],
   "source": [
    "# Example: Collect data for a \"Wave\" gesture\n",
    "# Replace \"wave\" with your desired gesture name\n",
    "data_wave, labels_wave = collect_training_data(gesture_name=\"\", num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combine All Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 samples from food_20250318_175210.pkl\n",
      "Loaded 100 samples from me_20250318_175321.pkl\n",
      "Loaded 100 samples from stop_20250318_175137.pkl\n",
      "Loaded 100 samples from sun_20250318_175233.pkl\n",
      "Loaded 100 samples from water_20250318_175100.pkl\n",
      "Loaded 100 samples from you_20250318_175340.pkl\n",
      "Combined dataset has 600 samples across 6 gesture classes\n"
     ]
    }
   ],
   "source": [
    "def combine_gesture_data(data_dir='gesture_data'):\n",
    "    \"\"\"Combine all gesture data files into a single dataset\"\"\"\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Error: Directory '{data_dir}' does not exist.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Get all pickle files in the directory\n",
    "    files = [f for f in os.listdir(data_dir) if f.endswith('.pkl')]\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No data files found in '{data_dir}'\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load and combine all data\n",
    "    for file in files:\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data_dict = pickle.load(f)\n",
    "                all_data.extend(data_dict['data'])\n",
    "                all_labels.extend(data_dict['labels'])\n",
    "            print(f\"Loaded {len(data_dict['labels'])} samples from {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "    \n",
    "    print(f\"Combined dataset has {len(all_data)} samples across {len(set(all_labels))} gesture classes\")\n",
    "    \n",
    "    return all_data, all_labels\n",
    "\n",
    "# Combine all collected data\n",
    "all_data, all_labels = combine_gesture_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM model...\n",
      "Test accuracy: 100.00%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        food       1.00      1.00      1.00        23\n",
      "          me       1.00      1.00      1.00        20\n",
      "        stop       1.00      1.00      1.00        19\n",
      "         sun       1.00      1.00      1.00        17\n",
      "       water       1.00      1.00      1.00        17\n",
      "         you       1.00      1.00      1.00        24\n",
      "\n",
      "    accuracy                           1.00       120\n",
      "   macro avg       1.00      1.00      1.00       120\n",
      "weighted avg       1.00      1.00      1.00       120\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to gesture_model_svm_20250318_175357.pkl\n",
      "Scaler saved to gesture_scaler_20250318_175357.pkl\n"
     ]
    }
   ],
   "source": [
    "def train_gesture_model(features, labels, model_type='svm'):\n",
    "    \"\"\"Train a model on the collected gesture data\"\"\"\n",
    "    # Convert data to numpy arrays\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train the model\n",
    "    if model_type.lower() == 'svm':\n",
    "        model = SVC(kernel='rbf', probability=True)\n",
    "        print(\"Training SVM model...\")\n",
    "    else:  # Default to Random Forest\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        print(\"Training Random Forest model...\")\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the model and scaler\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_filename = f\"gesture_model_{model_type}_{timestamp}.pkl\"\n",
    "    scaler_filename = f\"gesture_scaler_{timestamp}.pkl\"\n",
    "    \n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    with open(scaler_filename, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "    print(f\"Scaler saved to {scaler_filename}\")\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "# Check if we have data to train on\n",
    "if all_data and all_labels:\n",
    "    # Train an SVM model\n",
    "    model, scaler = train_gesture_model(all_data, all_labels, model_type='svm')\n",
    "else:\n",
    "    print(\"No training data available. Please collect data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-time Gesture Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 117\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# def recognize_gestures(model=None, scaler=None, confidence_threshold=0.7):\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#     \"\"\"Real-time gesture recognition using the trained model\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     # If no model is provided, try to load the latest one\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Release resources\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cap' is not defined"
     ]
    }
   ],
   "source": [
    "def recognize_gestures(model=None, scaler=None, confidence_threshold=0.7):\n",
    "    \"\"\"Real-time gesture recognition using the trained model\"\"\"\n",
    "    # If no model is provided, try to load the latest one\n",
    "    if model is None or scaler is None:\n",
    "        # Find the latest model and scaler files\n",
    "        model_files = [f for f in os.listdir('.') if f.startswith('gesture_model_')]\n",
    "        scaler_files = [f for f in os.listdir('.') if f.startswith('gesture_scaler_')]\n",
    "        \n",
    "        if not model_files or not scaler_files:\n",
    "            print(\"No model or scaler files found. Please train a model first.\")\n",
    "            return\n",
    "        \n",
    "        latest_model = max(model_files)\n",
    "        latest_scaler = max(scaler_files)\n",
    "        \n",
    "        try:\n",
    "            with open(latest_model, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            with open(latest_scaler, 'rb') as f:\n",
    "                scaler = pickle.load(f)\n",
    "            print(f\"Loaded model from {latest_model} and scaler from {latest_scaler}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model or scaler: {e}\")\n",
    "            return\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Initialize MediaPipe\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, \\\n",
    "         mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Failed to capture image from webcam.\")\n",
    "                break\n",
    "                \n",
    "            # Flip the image horizontally for a selfie-view display\n",
    "            image = cv2.flip(image, 1)\n",
    "            \n",
    "            # Convert the BGR image to RGB\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process the image for pose and hand detection\n",
    "            pose_results = pose.process(image_rgb)\n",
    "            hands_results = hands.process(image_rgb)\n",
    "            \n",
    "            # Copy original image for display\n",
    "            display_image = image.copy()\n",
    "            \n",
    "            # Create a combined result object\n",
    "            combined_results = type('CombinedResults', (), {\n",
    "                'pose_landmarks': pose_results.pose_landmarks if pose_results.pose_landmarks else None,\n",
    "                'multi_hand_landmarks': hands_results.multi_hand_landmarks if hands_results.multi_hand_landmarks else None,\n",
    "                'multi_handedness': hands_results.multi_handedness if hands_results.multi_handedness else None\n",
    "            })\n",
    "            \n",
    "            # Extract landmarks\n",
    "            features = extract_landmarks(combined_results)\n",
    "            \n",
    "            # Normalize landmarks\n",
    "            normalized_features = normalize_landmarks(features)\n",
    "            \n",
    "            # Make prediction if we have valid landmarks\n",
    "            prediction = \"No gesture detected\"\n",
    "            confidence = 0.0\n",
    "            \n",
    "            if pose_results.pose_landmarks or hands_results.multi_hand_landmarks:\n",
    "                # Scale features\n",
    "                scaled_features = scaler.transform([normalized_features])\n",
    "                \n",
    "                # Get prediction and confidence\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    proba = model.predict_proba(scaled_features)[0]\n",
    "                    confidence = np.max(proba)\n",
    "                    if confidence >= confidence_threshold:\n",
    "                        prediction = model.predict(scaled_features)[0]\n",
    "                    else:\n",
    "                        prediction = \"Uncertain gesture\"\n",
    "                else:\n",
    "                    prediction = model.predict(scaled_features)[0]\n",
    "                    confidence = 1.0  # No confidence score available\n",
    "            \n",
    "            # Draw landmarks on the image\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    display_image,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=pose_drawing_spec,\n",
    "                    connection_drawing_spec=connection_drawing_spec)\n",
    "            \n",
    "            if hands_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        display_image,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        landmark_drawing_spec=hand_drawing_spec,\n",
    "                        connection_drawing_spec=connection_drawing_spec)\n",
    "            \n",
    "            # Display prediction\n",
    "            cv2.putText(display_image, f\"Gesture: {prediction}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(display_image, f\"Confidence: {confidence:.2f}\", (10, 70),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show the image\n",
    "            cv2.imshow('Gesture Recognition', display_image)\n",
    "            \n",
    "            # Exit if ESC key is pressed\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\aiml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run real-time gesture recognition using the trained model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# If you've just trained a model in this session, you can pass it directly\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[1;32m----> 4\u001b[0m     recognize_gestures(model\u001b[38;5;241m=\u001b[39mmodel, scaler\u001b[38;5;241m=\u001b[39mscaler)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Otherwise it will try to load the latest saved model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     recognize_gestures()\n",
      "Cell \u001b[1;32mIn[16], line 113\u001b[0m, in \u001b[0;36mrecognize_gestures\u001b[1;34m(model, scaler, confidence_threshold)\u001b[0m\n\u001b[0;32m    110\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGesture Recognition\u001b[39m\u001b[38;5;124m'\u001b[39m, display_image)\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;66;03m# Exit if ESC key is pressed\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m:\n\u001b[0;32m    114\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Release resources\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run real-time gesture recognition using the trained model\n",
    "# If you've just trained a model in this session, you can pass it directly\n",
    "if 'model' in globals() and 'scaler' in globals():\n",
    "    recognize_gestures(model=model, scaler=scaler)\n",
    "else:\n",
    "    # Otherwise it will try to load the latest saved model\n",
    "    recognize_gestures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(features, labels, model_type='svm'):\n",
    "    \"\"\"Evaluate model performance using cross-validation\"\"\"\n",
    "    from sklearn.model_selection import cross_val_score, KFold\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    \n",
    "    # Convert data to numpy arrays\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Create the model\n",
    "    if model_type.lower() == 'svm':\n",
    "        model = SVC(kernel='rbf', probability=True)\n",
    "        print(\"Evaluating SVM model...\")\n",
    "    else:  # Default to Random Forest\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        print(\"Evaluating Random Forest model...\")\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    print(f\"Cross-validation accuracy: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "    \n",
    "    # Train on full dataset for feature importance (if Random Forest)\n",
    "    model.fit(X_scaled, y)\n",
    "    \n",
    "    # Split data for confusion matrix\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # If Random Forest, show feature importance\n",
    "    if model_type.lower() == 'random_forest':\n",
    "        # Get feature importance\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.title('Feature Importance')\n",
    "        plt.bar(range(20), importances[indices[:20]], align='center')\n",
    "        plt.xticks(range(20), [f'Feature {i}' for i in indices[:20]], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print most important features\n",
    "        print(\"\\nTop 10 most important features:\")\n",
    "        for i in range(10):\n",
    "            print(f\"Feature {indices[i]}: {importances[indices[i]]:.4f}\")\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "# Check if we have data to evaluate\n",
    "if 'all_data' in globals() and 'all_labels' in globals() and all_data and all_labels:\n",
    "    # Example: Evaluate Random Forest model\n",
    "    evaluate_model_performance(all_data, all_labels, model_type='random_forest')\n",
    "else:\n",
    "    print(\"No data available for evaluation. Please collect and combine data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gesture_data(features, labels):\n",
    "    \"\"\"Visualize the collected gesture data using dimensionality reduction\"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Convert data to numpy arrays\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Perform PCA for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot PCA results\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis', s=100, alpha=0.8)\n",
    "    plt.title('PCA Visualization of Gesture Data')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Perform t-SNE for better visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X) // 5))\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot t-SNE results\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y, palette='viridis', s=100, alpha=0.8)\n",
    "    plt.title('t-SNE Visualization of Gesture Data')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate explained variance for PCA\n",
    "    pca = PCA().fit(X_scaled)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Explained Variance by PCA Components')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Check if we have data to visualize\n",
    "if 'all_data' in globals() and 'all_labels' in globals() and all_data and all_labels:\n",
    "    visualize_gesture_data(all_data, all_labels)\n",
    "else:\n",
    "    print(\"No data available for visualization. Please collect and combine data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save and Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, scaler, model_name='gesture_model'):\n",
    "    \"\"\"Save the trained model and scaler\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_filename = f\"{model_name}_{timestamp}.pkl\"\n",
    "    scaler_filename = f\"{model_name}_scaler_{timestamp}.pkl\"\n",
    "    \n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    with open(scaler_filename, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "    print(f\"Scaler saved to {scaler_filename}\")\n",
    "    \n",
    "    return model_filename, scaler_filename\n",
    "\n",
    "def load_model(model_filename, scaler_filename):\n",
    "    \"\"\"Load a trained model and scaler\"\"\"\n",
    "    try:\n",
    "        with open(model_filename, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        with open(scaler_filename, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        \n",
    "        print(f\"Model loaded from {model_filename}\")\n",
    "        print(f\"Scaler loaded from {scaler_filename}\")\n",
    "        \n",
    "        return model, scaler\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or scaler: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage:\n",
    "# If you have a trained model and want to save it:\n",
    "if 'model' in globals() and 'scaler' in globals():\n",
    "    save_model(model, scaler, model_name='my_custom_gesture_model')\n",
    "\n",
    "# To load a specific model:\n",
    "# model, scaler = load_model('my_custom_gesture_model_20250318_123456.pkl', 'my_custom_gesture_model_scaler_20250318_123456.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Gesture Recording Utility\n",
    "\n",
    "This utility helps you record a sequence of gestures with timestamps, which could be useful for creating gesture sequences or commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_gesture_sequence(model=None, scaler=None, duration=30, confidence_threshold=0.7):\n",
    "    \"\"\"Record a sequence of gestures over time\"\"\"\n",
    "    # If no model is provided, try to load the latest one\n",
    "    if model is None or scaler is None:\n",
    "        # Find the latest model and scaler files\n",
    "        model_files = [f for f in os.listdir('.') if f.startswith('gesture_model_')]\n",
    "        scaler_files = [f for f in os.listdir('.') if f.startswith('gesture_scaler_')]\n",
    "        \n",
    "        if not model_files or not scaler_files:\n",
    "            print(\"No model or scaler files found. Please train a model first.\")\n",
    "            return\n",
    "        \n",
    "        latest_model = max(model_files)\n",
    "        latest_scaler = max(scaler_files)\n",
    "        \n",
    "        try:\n",
    "            with open(latest_model, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            with open(latest_scaler, 'rb') as f:\n",
    "                scaler = pickle.load(f)\n",
    "            print(f\"Loaded model from {latest_model} and scaler from {latest_scaler}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model or scaler: {e}\")\n",
    "            return\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Initialize variables for recording\n",
    "    gesture_sequence = []\n",
    "    start_time = time.time()\n",
    "    last_gesture = None\n",
    "    last_gesture_time = start_time\n",
    "    gesture_duration = 0\n",
    "    \n",
    "    # Initialize MediaPipe\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, \\\n",
    "         mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Failed to capture image from webcam.\")\n",
    "                break\n",
    "                \n",
    "            # Check if recording time is up\n",
    "            current_time = time.time()\n",
    "            elapsed_time = current_time - start_time\n",
    "            if elapsed_time >= duration:\n",
    "                break\n",
    "            \n",
    "            # Flip the image horizontally for a selfie-view display\n",
    "            image = cv2.flip(image, 1)\n",
    "            \n",
    "            # Convert the BGR image to RGB\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process the image for pose and hand detection\n",
    "            pose_results = pose.process(image_rgb)\n",
    "            hands_results = hands.process(image_rgb)\n",
    "            \n",
    "            # Copy original image for display\n",
    "            display_image = image.copy()\n",
    "            \n",
    "            # Create a combined result object\n",
    "            combined_results = type('CombinedResults', (), {\n",
    "                'pose_landmarks': pose_results.pose_landmarks if pose_results.pose_landmarks else None,\n",
    "                'multi_hand_landmarks': hands_results.multi_hand_landmarks if hands_results.multi_hand_landmarks else None,\n",
    "                'multi_handedness': hands_results.multi_handedness if hands_results.multi_handedness else None\n",
    "            })\n",
    "            \n",
    "            # Extract landmarks\n",
    "            features = extract_landmarks(combined_results)\n",
    "            \n",
    "            # Normalize landmarks\n",
    "            normalized_features = normalize_landmarks(features)\n",
    "            \n",
    "            # Make prediction if we have valid landmarks\n",
    "            prediction = \"No gesture detected\"\n",
    "            confidence = 0.0\n",
    "            \n",
    "            if pose_results.pose_landmarks or hands_results.multi_hand_landmarks:\n",
    "                # Scale features\n",
    "                scaled_features = scaler.transform([normalized_features])\n",
    "                \n",
    "                # Get prediction and confidence\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    proba = model.predict_proba(scaled_features)[0]\n",
    "                    confidence = np.max(proba)\n",
    "                    if confidence >= confidence_threshold:\n",
    "                        prediction = model.predict(scaled_features)[0]\n",
    "                    else:\n",
    "                        prediction = \"Uncertain gesture\"\n",
    "                else:\n",
    "                    prediction = model.predict(scaled_features)[0]\n",
    "                    confidence = 1.0  # No confidence score available\n",
    "            \n",
    "            # Update gesture sequence\n",
    "            if prediction != \"No gesture detected\" and prediction != \"Uncertain gesture\":\n",
    "                if prediction != last_gesture:\n",
    "                    # If we have a previous gesture, record its duration\n",
    "                    if last_gesture is not None:\n",
    "                        gesture_duration = current_time - last_gesture_time\n",
    "                        gesture_sequence.append({\n",
    "                            'gesture': last_gesture,\n",
    "                            'start_time': last_gesture_time - start_time,\n",
    "                            'duration': gesture_duration,\n",
    "                            'confidence': confidence\n",
    "                        })\n",
    "                    \n",
    "                    # Update to new gesture\n",
    "                    last_gesture = prediction\n",
    "                    last_gesture_time = current_time\n",
    "            \n",
    "            # Draw landmarks on the image\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    display_image,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=pose_drawing_spec,\n",
    "                    connection_drawing_spec=connection_drawing_spec)\n",
    "            \n",
    "            if hands_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        display_image,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        landmark_drawing_spec=hand_drawing_spec,\n",
    "                        connection_drawing_spec=connection_drawing_spec)\n",
    "            \n",
    "            # Display prediction and recording info\n",
    "            cv2.putText(display_image, f\"Gesture: {prediction}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(display_image, f\"Confidence: {confidence:.2f}\", (10, 70),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(display_image, f\"Recording: {elapsed_time:.1f}s / {duration}s\", (10, 110),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            \n",
    "            # Show the image\n",
    "            cv2.imshow('Gesture Recording', display_image)\n",
    "            \n",
    "            # Exit if ESC key is pressed\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "    \n",
    "    # Record the last gesture if there is one\n",
    "    if last_gesture is not None:\n",
    "        gesture_duration = time.time() - last_gesture_time\n",
    "        gesture_sequence.append({\n",
    "            'gesture': last_gesture,\n",
    "            'start_time': last_gesture_time - start_time,\n",
    "            'duration': gesture_duration,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Print and save the recorded sequence\n",
    "    print(\"\\nRecorded Gesture Sequence:\")\n",
    "    for i, entry in enumerate(gesture_sequence):\n",
    "        print(f\"{i+1}. {entry['gesture']} (Start: {entry['start_time']:.2f}s, Duration: {entry['duration']:.2f}s, Confidence: {entry['confidence']:.2f})\")\n",
    "    \n",
    "    # Save sequence to file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    sequence_filename = f\"gesture_sequence_{timestamp}.pkl\"\n",
    "    \n",
    "    with open(sequence_filename, 'wb') as f:\n",
    "        pickle.dump(gesture_sequence, f)\n",
    "    \n",
    "    print(f\"\\nSequence saved to {sequence_filename}\")\n",
    "    \n",
    "    return gesture_sequence\n",
    "\n",
    "# Example: Record a 20-second gesture sequence\n",
    "# gesture_sequence = record_gesture_sequence(duration=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Instructions for Use\n",
    "\n",
    "1. **Collect Training Data**:\n",
    "   - Define what gestures you want to recognize\n",
    "   - For each gesture, run the `collect_training_data()` function with the gesture name and number of samples\n",
    "   - Make sure to vary your position, lighting, and slight variations in the gesture for better generalization\n",
    "\n",
    "2. **Combine and Process Data**:\n",
    "   - Run `combine_gesture_data()` to merge all your collected data\n",
    "\n",
    "3. **Train the Model**:\n",
    "   - Use `train_gesture_model()` with the combined data\n",
    "   - Try both SVM and Random Forest to see which performs better\n",
    "\n",
    "4. **Test in Real-Time**:\n",
    "   - Run `recognize_gestures()` to test your model with live webcam input\n",
    "\n",
    "5. **Evaluate and Refine**:\n",
    "   - Use `evaluate_model_performance()` and `visualize_gesture_data()` to understand your model's strengths and weaknesses\n",
    "   - If needed, collect more data for gestures that are frequently misclassified\n",
    "\n",
    "6. **Save and Share Your Model**:\n",
    "   - Use `save_model()` to export your trained model\n",
    "   - The model can be loaded later using `load_model()`\n",
    "\n",
    "7. **Record Gesture Sequences**:\n",
    "   - Once your model is working well, you can record sequences of gestures using `record_gesture_sequence()`\n",
    "   - These can be used for creating gesture-based commands or interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Workflow\n",
    "\n",
    "Here's a complete example workflow for creating a gesture recognition system that can detect three gestures: \"wave\", \"thumbs_up\", and \"peace\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Collect data for three different gestures\n",
    "data_wave, labels_wave = collect_training_data(gesture_name=\"wave\", num_samples=50)\n",
    "data_thumbs_up, labels_thumbs_up = collect_training_data(gesture_name=\"thumbs_up\", num_samples=50)\n",
    "data_peace, labels_peace = collect_training_data(gesture_name=\"peace\", num_samples=50)\n",
    "\n",
    "# 2. Combine all collected data\n",
    "all_data, all_labels = combine_gesture_data()\n",
    "\n",
    "# 3. Visualize the data\n",
    "visualize_gesture_data(all_data, all_labels)\n",
    "\n",
    "# 4. Train and evaluate an SVM model\n",
    "model_svm, scaler_svm = train_gesture_model(all_data, all_labels, model_type='svm')\n",
    "\n",
    "# 5. Train and evaluate a Random Forest model\n",
    "model_rf, scaler_rf = train_gesture_model(all_data, all_labels, model_type='random_forest')\n",
    "\n",
    "# 6. Save the best performing model\n",
    "# Assume the SVM model performed better\n",
    "model_file, scaler_file = save_model(model_svm, scaler_svm, model_name='gesture_recognition_v1')\n",
    "\n",
    "# 7. Test the model in real-time\n",
    "recognize_gestures(model=model_svm, scaler=scaler_svm)\n",
    "\n",
    "# 8. Record a sequence of gestures (optional)\n",
    "gesture_sequence = record_gesture_sequence(model=model_svm, scaler=scaler_svm, duration=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
